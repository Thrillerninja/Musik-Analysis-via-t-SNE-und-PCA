%!TEX root = ../main.tex

\chapter{Abstract}
Aktivierungsfunktionen sind essenziell für die Leistungsfähigkeit neuronaler Netzwerke, da sie Nichtlinearitäten einführen und die Verarbeitung komplexer Muster ermöglichen. Diese Arbeit untersucht zwei prominente Aktivierungsfunktionen: Sigmoid und ReLU. Ziel ist es, ihre Eigenschaften und ihr Verhalten in verschiedenen Szenarien des maschinellen Lernens systematisch zu vergleichen. Der Fokus liegt dabei auf zentralen Attributen wie Trainingsgeschwindigkeit, Genauigkeit (Accuracy), Präzision, Verhalten bei Gradientenproblemen und Rechenaufwand.
Die Untersuchung zeigt, dass ReLU durch die Einfachheit und Effizienz in vielen modernen Anwendungen die bevorzugte Wahl ist, insbesondere in DNNs, die von Problemen wie dem Vanishing Gradient betroffen sind. Sigmoid weist, trotz der eingeschränkten Verwendung in heutigen Architekturen, Stärken in spezifischen Anwendungsbereichen auf, die näher beleuchtet werden.
Die Ergebnisse dieser Arbeit liefern eine fundierte Grundlage für die Wahl der Aktivierungsfunktion in maschinellen Lernsystemen und verdeutlichen die entscheidende Rolle, die diese für die Leistungsfähigkeit und Effizienz neuronaler Netzwerke spielt.

\textit{
Activation functions are essential for the performance of neural networks as they introduce non-linearities and enable the processing of complex patterns. This study examines two prominent activation functions: Sigmoid and ReLU. The aim is to systematically compare their properties and behavior across various machine learning scenarios. The focus is on key attributes such as training speed, accuracy, precision, behavior in gradient-related issues, and computational cost.
The analysis shows that ReLU is the preferred choice in many modern applications due to its simplicity and efficiency, especially in deep neural networks (DNNs) affected by challenges like the vanishing gradient problem. Despite its limited use in contemporary architectures, Sigmoid demonstrates strengths in specific application areas, which are explored in detail.
The findings of this study provide a solid foundation for selecting activation functions in machine learning systems and highlight their critical role in determining the performance and efficiency of neural networks.
}




