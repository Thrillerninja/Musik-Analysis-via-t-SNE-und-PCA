\chapter{Beispielumsetzung}

Nun, da alles bereit ist, könen wir die \ac{PCA} und \ac{t-SNE} Ansätze zur Dimensionsreduktion von Musikdaten vergleichen. Zuerst werden wir den Datensatz in die Datenbank laden und die Daten normalisieren. Dann wenden wir zuerst die PCA und dann t-SNE auf die Musikdaten an und analysieren die Ergebnisse.

\section{Datenvorbereitung}

Bevor die Dimensionsreduktion durchgeführt werden kann, müssen die Musikdaten zunächst in die Datenbank geladen und anschließend normalisiert werden.

\textbf{Normalisierung der Daten}

Die Merkmale der Musikdaten, wie Lautstärke, Energie, Tanzbarkeit und Dauer, liegen auf unterschiedlichen Skalen vor. Um eine konsistente Analyse zu ermöglichen, ist eine Normalisierung der Daten erforderlich. Hierfür wird der StandardScaler aus der Python-Bibliothek sklearn verwendet, der die Daten so transformiert, dass sie eine Standardnormalverteilung mit einem Mittelwert von 0 und einer Standardabweichung von 1 aufweisen.

\textbf{Datenimport in die Datenbank}

Die Musikdaten aus der Datei \texttt{spotify\_dataset.csv} werden in die PostgreSQL-Datenbank importiert. Dazu wird die Tabelle \texttt{music\_vectors} mittels der \texttt{init.sql} Datei erstellt, die die Merkmale der Musikstücke als Vektoren speichert. Der folgende Python-Code zeigt, wie die Daten in die Datenbank eingefügt werden:

\begin{lstlisting}[language=python, caption={Insert der Musikdaten in die Datenbank}]
conn = connect_to_postgres()
execute_values(
    cursor,
    "INSERT INTO music_vectors (features) VALUES %s",
    data_to_insert,
    template="(%s)"
)
conn.commit()
\end{lstlisting}

Nach dem erfolgreichen Import stehen die Daten für die weitere Verarbeitung und Analyse bereit.






\section{Dimensionalitätsreduktion mit PCA}

Die Principal Component Analysis (PCA) ist eine Methode zur Dimensionsreduktion, die darauf abzielt, die Hauptvariationsrichtungen in den Daten zu identifizieren. Sie reduziert die Anzahl der Dimensionen, indem sie die Daten auf eine kleinere Anzahl von Hauptkomponenten projiziert, die die maximale Varianz erklären.

\textbf{Anwendung von PCA}

Das folgende Python-Skript zeigt, wie PCA mit hilfe der sklearn lib auf die normalisierten Daten angewendet wird:

\begin{lstlisting}[language=python, caption={PCA-Analyse der Musikdaten}]
from sklearn.decomposition import PCA
# Perform PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(features_standardized)
\end{lstlisting}

Dabei ergibt sich die \texttt{pca\_result} als zweidimensionales Array, das die reduzierten Daten enthält. Bei dem genutzten Beispieldatensatz ergibt sich eine Varianz von 31\% für die erste und 13\% für die zweite Hauptkomponente. Damit konnten rund 45\% der Varianz im vorliegenden Datensatz abgedeckt werden.

Dies zeigt sich in der Visualisierung der Dimensionsreduktion in der die Datenpunkte der Klassen zwar in zwei Regionen liegen, diese jedoch aneinander grenzen.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{resources/images/pca_visualization.png}
    \caption{PCA der Musikdaten}
\end{figure}

Ohne die Einfärbung wäre die Unterteilung in Cluster nicht eindeutig zu erkennen. Damit wäre eine Klassifizierung neuer Datenpunkte abhängig von der Position nicht zuverlässig möglich. Für Songs, die stark in einem roten/blauen Bereich liegen, wäre eine Klassifizierung jedoch möglich.

Was der Grund für eine solche Unterteilung ist, kann durch die Analyse der Hauptkomponenten ermittelt werden.
Die PCA-Loadings geben an, wie stark jedes Feature zu den Hauptkomponenten beiträgt. Sie sind ein entscheidender Bestandteil der PCA-Analyse, da sie helfen, die Bedeutung der einzelnen Features für die Hauptkomponenten zu interpretieren. Die folgende Abbildung zeigt die Beiträge der Features zu den beiden Hauptkomponenten:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{resources/images/pca_loadings.png}
    \caption{Erklärte Varianz der Hauptkomponenten}
\end{figure}

\textbf{Interpretation der PCA-Loadings}

Die Balkendiagramme zeigen die Richtung und Stärke der Beiträge der einzelnen Features zu den Hauptkomponenten. Positive/negative Werte geben an, ob ein Feature positiv/negativ mit der jeweiligen Hauptkomponente korreliert. Die Länge der Balken zeigt die Stärke des Beitrags an.

In PC1 sind somit besonders die Features \texttt{instrumentalness} und \texttt{acousticness} stark positiv vertreten. In PC2 sind es \texttt{energy}, \texttt{loudness} und \texttt{lieveness}.









\section{Dimensionalitätsreduktion mit t-SNE}

Da t-SNE eine weitere Methode zur Dimensionsreduktion mit dem fokus auf die Erhaltung von lokalen Strukturen ist, wenden wir diese auf die normalisierten Daten an.

\textbf{Anwendung von t-SNE}

\begin{lstlisting}[language=python, caption={t-SNE-Analyse der Musikdaten}]
    from sklearn.decomposition import PCA
    # Perform t-SNE
    tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=1000)
    tsne_result = tsne.fit_transform(features_standardized)
\end{lstlisting}

Bei der t-SNE-Analyse wird die Perplexität als neuen Parameter eingeführt. Dieser Wert gibt an, wie viele Nachbarn für die Berechnung der Wahrscheinlichkeitsverteilung berücksichtigt werden sollen. Ein hoher Wert führt zu einer stärkeren Betonung globaler Strukturen, während ein niedriger Wert lokale Strukturen bevorzugt.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{resources/images/tsne_perplexity_comparison.png}
    \caption{Verschiedene Perplexitätswerte für t-SNE}
\end{figure}
Als optimalen Wert für die Perplexität wird 30 gewählt, da dieser relativ gut getrennte Cluster zeigt, und Probleme mit hohen Werten wie das Zusammenfassen von Clustern verhindert.