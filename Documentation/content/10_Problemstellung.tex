%!TEX root = ../main.tex

\chapter{Einleitung}

\section{Hinführung zum Thema}
In der heutigen datenlastigen Welt stehen wir immer häufiger vor der Herausforderung, hochdimensionale Daten zu analysieren und zu verstehen. Ein gutes Beispiel dafür ist der Bereich der Musikanalyse, in dem wir Datensätzen begegnen, die durch zahlreiche Merkmale wie Tanzbarkeit, Tempo, Lautstärke und viele weitere Attribute charakterisiert sind und darüber vorschläge erstellt werden. Diese multidimensionalen Merkmalsräume entziehen sich jedoch unserer direkten visuellen Wahrnehmung, die auf zwei oder maximal drei Dimensionen beschränkt ist.

Dimensionalitätsreduktionsmethoden wie \acf{t-SNE} \cite{vanDerMaaten2008} und \acf{PCA} \cite{Jolliffe2002} bieten einen Ausweg aus diesem Dilemma. Sie ermöglichen es uns, die wesentlichen Strukturen und Muster hochdimensionaler Datensätze in einem niedrigdimensionalen Raum darzustellen, ohne dabei kritische Informationen zu verlieren. Diese Techniken haben sich als unverzichtbare Werkzeuge in der explorativen Datenanalyse etabliert und finden besonders in der Musikinformatik breite Anwendung.awdawdawdawd

Die moderne Musikindustrie nutzt solche Techniken bereits intensiv für Empfehlungssysteme, Genreklassifikationen und die Entdeckung musikalischer Trends. Streaming-Dienste wie Spotify verwenden ähnliche Ansätze, um Benutzern personalisierte Playlists zu erstellen und neue Musik zu empfehlen, die ihren Vorlieben entspricht.

\section{\acf{PCA}}

Principal Component Analysis ist eine der ältesten und am häufigsten verwendeten Techniken zur Dimensionalitätsreduktion. Sie wurde bereits Anfang des 20. Jahrhunderts entwickelt und hat sich seither als Standard-Methode etabliert \cite{Pearson1901}.

\subsubsection{Mathematische Grundlage}

PCA basiert auf einer linearen Transformation der Daten in ein neues Koordinatensystem, bei dem die Achsen (Hauptkomponenten) nach absteigender Varianz geordnet sind. Die erste Hauptkomponente erklärt den größten Teil der Varianz in den Daten, die zweite den zweitgrößten und so weiter.

Mathematisch gesehen sucht PCA nach den Eigenvektoren der Kovarianzmatrix der Daten:

\[
\text{Cov}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T
\]

Die Eigenvektoren bilden die Hauptkomponenten, und die zugehörigen Eigenwerte geben an, wie viel Varianz durch jede Komponente erklärt wird.\cite{Pearson1901, pcaTut}

\subsubsection{Eigenschaften und Anwendung}

PCA eignet sich besonders gut für Datensätze mit linearen Strukturen und wird oft als erster Schritt in einer Datenanalyse verwendet. Ihre Stärken liegen in der Einfachheit, Interpretierbarkeit und Skalierbarkeit:

\begin{itemize}
    \item \textbf{Einfachheit}: PCA ist konzeptionell leicht zu verstehen und effizient zu berechnen.
    \item \textbf{Interpretierbarkeit}: Die Hauptkomponenten können oft semantisch interpretiert werden.
    \item \textbf{Rauschreduktion}: Durch Fokussierung auf die Komponenten mit der größten Varianz wird Rauschen reduziert.
    \item \textbf{Datenkompression}: PCA eignet sich hervorragend zur Datenkompression mit minimalem Informationsverlust.
\end{itemize}

Im Kontext der Musikdatenanalyse kann PCA helfen, Zusammenhänge zwischen verschiedenen Merkmalen wie Tempo, Lautstärke und Tanzbarkeit zu identifizieren und Musikstücke in einem zweidimensionalen Raum zu visualisieren.

\section{\acf{t-SNE}}

t-SNE ist eine neuere, nichtlineare Dimensionalitätsreduktionstechnik, die 2008 von Laurens van der Maaten und Geoffrey Hinton entwickelt wurde \cite{vanDerMaaten2008}. Im Gegensatz zu PCA konzentriert sich t-SNE darauf, die lokale Struktur der Daten zu bewahren, was sie besonders wertvoll für die Visualisierung macht.

\subsection{Mathematischer Hintergrund}

t-SNE arbeitet in zwei Hauptschritten:

\begin{enumerate}
    \item \textbf{Modellierung der Ähnlichkeiten im hochdimensionalen Raum}: t-SNE konstruiert eine Wahrscheinlichkeitsverteilung über Paare von Datenpunkten, sodass ähnliche Objekte eine hohe Wahrscheinlichkeit haben, als Nachbarn ausgewählt zu werden.
    \item \textbf{Minimierung der Kullback-Leibler-Divergenz}: t-SNE erzeugt dann eine entsprechende Wahrscheinlichkeitsverteilung im niedrigdimensionalen Raum und versucht, die Kullback-Leibler-Divergenz zwischen den beiden Verteilungen zu minimieren:
    \[
    \text{KL}(P || Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
    \]
    Dabei verwendet t-SNE eine t-Verteilung im niedrigdimensionalen Raum, um das ``Crowding Problem'' zu mildern, das bei anderen Methoden auftreten kann.
\end{enumerate}\cite{vanDerMaaten2008}

\subsection{Eigenschaften und Anwendung}

t-SNE zeichnet sich durch folgende Eigenschaften aus:

\begin{itemize}
    \item \textbf{Bewahrt lokale Strukturen}: t-SNE ist besonders gut darin, lokale Nachbarschaftsbeziehungen zu erhalten.
    \item \textbf{Gruppierungsfähigkeit}: t-SNE tendiert dazu, ähnliche Datenpunkte zu Clustern zusammenzufassen.
    \item \textbf{Nichtlinearität}: Kann komplexe, nichtlineare Muster erfassen, die PCA möglicherweise verpasst.
    \item \textbf{Perplexitätsparameter}: Erlaubt eine Feinabstimmung der Balance zwischen lokalen und globalen Aspekten der Daten.
\end{itemize}

In der Musikanalyse kann t-SNE verwendet werden, um Musikstücke basierend auf ihrer akustischen Ähnlichkeit zu visualisieren, was oft zu intuitiv verständlichen Clustern führt, die verschiedene Genres oder Stile repräsentieren.\cite{tsnescikit, efficienttsne}

\section{Vektordatenbanken und pgvector}

Die Verwaltung und Abfrage hochdimensionaler Vektordaten stellt besondere Anforderungen an Datenbanksysteme. Herkömmliche relationale Datenbanken sind nicht optimal für die effiziente Ähnlichkeitssuche in Vektorräumen konzipiert. Hier kommen spezialisierte Vektordatenbanken ins Spiel.

\textbf{Was ist pgvector?}

pgvector ist eine Erweiterung für das populäre PostgreSQL-Datenbanksystem, die es ermöglicht, Vektoren effizient zu speichern und Ähnlichkeitsabfragen durchzuführen \cite{pgvector}. Es unterstützt verschiedene Distanzmetriken wie euklidische Distanz, Kosinus-Ähnlichkeit und Taxicab-Distanz.

Die Hauptfunktionen von pgvector umfassen:

\begin{itemize}
    \item Speicherung von dichten Vektoren unterschiedlicher Dimension.
    \item Effiziente Ähnlichkeitssuche mit verschiedenen Metriken.
    \item Indizierungsmethoden für beschleunigte Abfragen.
    \item Integration in das SQL-Ökosystem von PostgreSQL.
\end{itemize}

Die Kombination von pgvector mit Dimensionalitätsreduktionstechniken wie \ac{PCA} und \ac{t-SNE} bietet leistungsstarke Werkzeuge für die Analyse und Visualisierung von Musikdaten.